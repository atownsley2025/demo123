<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>ZeroSkid ‚Äì Technical Approach & Results</title>
  <link rel="stylesheet" href="style.css" />
</head>
<body>
  <header class="site-header">
    <div class="logo">ZeroSkid</div>
    <nav>
      <a href="index.html">Problem</a>
      <a href="technical.html" class="active">Technical Approach</a>
    </nav>
  </header>

  <main class="container">
    <section class="hero">
      <h1>Technical Approach, Experiments & Results</h1>
      <p>
        This page summarizes how I used AlexNet (and ResNet18) on my wet vs dry road dataset, how I set up training in
        Google Colab, and what I learned from the experiments.
      </p>
    </section>

    <section class="card" id="technical">
      <h2>üß† Model & Training Setup</h2>
      <p>
        I used <strong>PyTorch in Google Colab</strong> and fine-tuned an AlexNet model that was originally trained on
        ImageNet. I adapted it to a binary classification task with two classes: wet and dry.
      </p>
      <p>The main steps were:</p>
      <ul>
        <li>Loading images from Google Drive using <code>ImageFolder</code></li>
        <li>Resizing all images to 224√ó224</li>
        <li>Normalizing with ImageNet mean and standard deviation</li>
        <li>Using random horizontal flips as data augmentation in some runs</li>
        <li>Training for multiple epochs and tracking performance in Weights &amp; Biases (wandb)</li>
      </ul>
      <p>
        I also ran at least one experiment with <strong>ResNet18</strong> to compare a deeper architecture with AlexNet
        on the same dataset.
      </p>
    </section>

    <section class="card">
      <h2>‚öôÔ∏è Hyperparameter Experiments</h2>
      <p>
        I ran several experiments that changed different parts of the training setup:
      </p>
      <ul>
        <li><strong>Batch size:</strong> tested more than one batch size (e.g., 16 vs 32)</li>
        <li><strong>Learning rate:</strong> compared 0.001 vs 0.0001</li>
        <li><strong>Augmentation:</strong> trained with and without random horizontal flips</li>
        <li><strong>Architecture:</strong> compared AlexNet to ResNet18</li>
      </ul>
      <p>
        Each run was tracked in wandb, which let me compare training vs validation accuracy and loss, and helped me see
        which settings actually improved performance instead of just overfitting.
      </p>
    </section>

    <section class="card" id="experiments">
      <h2>üß™ Key Experiments & Visualizations</h2>

      <h3>Experiment 1 ‚Äì Baseline AlexNet</h3>
      <p>
        This was my baseline configuration:
      </p>
      <ul>
        <li>Model: AlexNet</li>
        <li>Batch size: 16</li>
        <li>Learning rate: 0.001</li>
        <li>Augmentation: OFF</li>
      </ul>
      <div class="grid">
        <div class="viz">
          <h3>Accuracy (Exp 1)</h3>
          <img src="images/exp1_accuracy.png" alt="Experiment 1 accuracy plot" />
        </div>
        <div class="viz">
          <h3>Loss (Exp 1)</h3>
          <img src="images/exp1_loss.png" alt="Experiment 1 loss plot" />
        </div>
      </div>
      <p class="note">
        In this run, training accuracy improved steadily, but validation accuracy didn‚Äôt follow as cleanly. This suggests
        the model started to overfit the small training set without augmentation.
      </p>

      <h3 style="margin-top:1.4rem;">Experiment 3 ‚Äì Best Performance</h3>
      <p>
        One of my stronger runs came from changing the learning rate and turning augmentation on:
      </p>
      <ul>
        <li>Model: AlexNet</li>
        <li>Batch size: 16</li>
        <li>Learning rate: 0.0001</li>
        <li>Augmentation: ON (random horizontal flips)</li>
      </ul>

      <div class="grid">
        <div class="viz">
          <h3>Accuracy (Exp 3)</h3>
          <img src="images/exp3_accuracy.png" alt="Experiment 3 accuracy plot" />
        </div>
        <div class="viz">
          <h3>Loss (Exp 3)</h3>
          <img src="images/exp3_loss.png" alt="Experiment 3 loss plot" />
        </div>
      </div>
      <p class="note">
        With a smaller learning rate and augmentation, training was smoother and validation accuracy improved. This
        shows that, even on a small dataset, tuning hyperparameters and using augmentation can make a noticeable
        difference.
      </p>

      <h3 style="margin-top:1.4rem;">Confusion Matrix (Best Model)</h3>
      <p>
        This confusion matrix comes from one of the best-performing runs:
      </p>
      <img src="images/confusion_matrix.png" alt="Confusion matrix for wet vs dry predictions" />
      <p class="note">
        The model is especially strong at recognizing <strong>wet</strong> roads. It sometimes predicts ‚Äúwet‚Äù when the
        actual label is dry. For a safety-focused system, this is not the worst trade-off, because false-wet is safer
        than false-dry.
      </p>
    </section>

    <section class="card" id="takeaways">
      <h2>üìå Takeaways</h2>
      <ul>
        <li>AlexNet can learn to separate wet vs dry roads even from a fairly small dataset.</li>
        <li>Lower learning rates and data augmentation led to better validation performance and smoother curves.</li>
        <li>Without augmentation, the model tended to overfit quickly.</li>
        <li>ResNet18 has more capacity, but with this dataset it can also overfit and doesn‚Äôt always beat AlexNet by a huge margin.</li>
        <li>The model‚Äôs habit of predicting ‚Äúwet‚Äù more aggressively is actually reasonable for a warning system.</li>
      </ul>
      <p style="margin-top:0.6rem;">
        Overall, this project is an early step toward the bigger ZeroSkid idea: using AI to give cars and drivers a better
        sense of road conditions before things go wrong.
      </p>
    </section>
  </main>

  <footer class="site-footer">
    <p>ZeroSkid ‚Äì Technical Approach ‚Ä¢ demo123</p>
  </footer>
</body>
</html>
